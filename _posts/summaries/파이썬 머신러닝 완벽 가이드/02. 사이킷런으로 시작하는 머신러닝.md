---
layout: post
title: 02. 사이킷런으로 시작하는 머신러닝
date: 2021-08-01 15:00:00 +09:00
modified: 
category: 파이썬 머심러닝 완벽 가이드
tags: [python]
image: "/assets/img/python_logo.png"
cover: ""
---

#### contents
[](#1. 사이킷런이란?)
[](#2. Estimator 및 fit(), perdict() 메서드)
[](#3. 사이킷런 주요 모듈 )
[](#4. train_test_split() )
[](#5. 교차검증)
[](# 5-1. K 폴드 교차검증)
[](# 5-2. Stratified K 폴드)
[](# 5-3. cross_val_score())
[](# 5-4. GridSearchCV)
[](#6. 데이터 인코딩 (원-핫 인코딩))
[](# 6-1. 레이블 인코딩)
[](# 6-2. 원-핫 인코딩)
[](#7. 피처 스케일링과 정규화)
[](# 7-1. StandardScaler)
[](# 7-2. MinMaxScaler)
[](# 7-3. 스케일링시 유의사항)

### 1. 사이킷런이란?

파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리<br>

**특징**<br>
1. 쉽고 가장 파이썬스러운 API 제공<br>
1. 머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API 제공<br>
1. 오랜 기간 실전 환경에서 검증되었으며, 매우 다양한 환경에서 사용되는 성숙한 라이브러리<br>

```
pip install scikit-learn
```

```
import sklearn
```

### 2. Estimator 및 fit(), perdict() 메서드
Estimator : 지도학습의 모든 알고리즘을 구현한 클래스를 통칭하는 이름<br>

fit() : Estimator 내부에 구현된 메서드로 ML 모델 학습에 사용<br>
predict() : Estimator 내부에 구현된 메서드로, ML 모델의 예측에 사용<br>

![estimator 이미지]()

### 3. 사이킷런 주요 모듈 

![사이킷런 주요 모듈 표 이미지]()

** 사이킷런 내장 데이터셋의 데이터는 넘파이 배열 타입이며, target_names, feature_names는 넘파이 배열 또는 파이썬 리스트 타입이다.<br>

### 4. train_test_split() 

`model_selection` 모듈에 포함되어 있고<br>
예측을 위해 학습용 데이터와 데스트 데이터를 분리하는 함수이다.<br>

##### 사용법

**train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)**

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

dt_clf = DecisionTreeClassifier( )
iris_data = load_iris()

X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, 
                                                    test_size=0.3, random_state=121)
```

* arrays : 피처 데이터 세트<br>
* arrays : 레이블 데이터 세트<br>
* test_size : 데이터 세트 샘플링 크기<br>
* train_size : 학습용 세트 샘플링 크기<br>
* shuffle : 데이터를 분리하기 전에 데이터를 섞을지/ 섞지 않을지<br>
* random_state : 호출할 때마다 동일한 학습/테스트 데이터를 생성하기 위해 주어지는 난수 값<br>

### 5. 교차검증

모델의 과적합을 막기 위해 모의고사를 여러번 수행하는 방법<br>

#### 5-1. K 폴드 교차검증

가장 보편적으로 사용되는 교차 검증 기법<br>
K 개의 데이터 폴드 세트를 만들어서 K번 만큼 학습과 검증 과정을 반복하는 방식<br>

##### 사용법
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import numpy as np

iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state=156)

# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.
kfold = KFold(n_splits=5)
cv_accuracy = []
print('붓꽃 데이터 세트 크기:',features.shape[0])

n_iter = 0

# KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환  
for train_index, test_index  in kfold.split(features):
    # kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    #학습 및 예측 
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)
    n_iter += 1
    # 반복 시 마다 정확도 측정 
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)
    
# 개별 iteration별 정확도를 합하여 평균 정확도 계산 
print('\n## 평균 검증 정확도:', np.mean(cv_accuracy))
```

#### 5-2. Stratified K 폴드

불균형한 분포를 가진 레이블 데이터를 위한 K 폴드 방법<br>
원본 데이터와 유사한 레이블 분포를 학습/테스트 데이터에 유지<br>

**왜곡된 레이블 데이터 세트에서는 반드시 Stratified K 폴드를 이용해야 한다.**<br>
**회귀(Regression)에서는 회구의 결정값이 이산값 형태의 레이블이 아니라 연속된 숫자 값이기 때문에 결정값 별로 푼포를 정하는 것이 의미가 없고 따라서 Stratified K 폴드가 지원되지 않는다.**<br>

##### 사용법
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold
import numpy as np

iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['label']=iris.target
iris_df['label'].value_counts()

skf = StratifiedKFold(n_splits=3)
n_iter=0

for train_index, test_index in skf.split(iris_df, iris_df['label']):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]
    print('## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())
    print('검증 레이블 데이터 분포:\n', label_test.value_counts())


dt_clf = DecisionTreeClassifier(random_state=156)

skfold = StratifiedKFold(n_splits=3)
n_iter=0
cv_accuracy=[]

# StratifiedKFold의 split( ) 호출시 반드시 레이블 데이터 셋도 추가 입력 필요  
for train_index, test_index  in skfold.split(features, label):
    # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    #학습 및 예측 
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)

    # 반복 시 마다 정확도 측정 
    n_iter += 1
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)
    
# 교차 검증별 정확도 및 평균 정확도 계산 
print('\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4))
print('## 평균 검증 정확도:', np.mean(cv_accuracy))
```

#### 5-3. cross_val_score()

교차 검증을 좀 더 편리하게 수행할 수 있도록 해주는 API<br>

1. 폴드 세트를 설정<br>
1. for 루프에서 반복적으로 학습 및 테스트 데이터 인덱스 추출<br>
1. 반복적으로 학습과 예측을 수행하고 예층 성능을 반환<br>

위 3단계를 한번에 진행하는 메서드이다.<br>

**내부적으로 StratifiedKFold를 사용한다.**<br>
**`cross_val_score()` 메서드는 하나의 평가 지표만 사용 가능하지만 비슷한 API로 `cross_validate()`가 있고 이 메서드는 여러개의 평가 지표를 반환할 수 있다.**

##### 사용법
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score , cross_validate
from sklearn.datasets import load_iris

iris_data = load_iris()
dt_clf = DecisionTreeClassifier(random_state=156)

data = iris_data.data
label = iris_data.target

# 성능 지표는 정확도(accuracy) , 교차 검증 세트는 3개 
scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3)
print('교차 검증별 정확도:',np.round(scores, 4))
print('평균 검증 정확도:', np.round(np.mean(scores), 4))
```

#### 5-4. GridSearchCV

교차 검증과 최적 하이퍼파라미터 튜닝을 한번에 하는 메서드이다.<br>

##### 사용법
```python

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# 데이터를 로딩하고 학습데이타와 테스트 데이터 분리
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, 
                                                    test_size=0.2, random_state=121)
dtree = DecisionTreeClassifier()

### parameter 들을 dictionary 형태로 설정
parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}


import pandas as pd

# param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정.  
### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴.  
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)

# 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 .
grid_dtree.fit(X_train, y_train)

# GridSearchCV 결과 추출하여 DataFrame으로 변환
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', \
           'split0_test_score', 'split1_test_score', 'split2_test_score']]

print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_))

# GridSearchCV의 refit으로 이미 학습이 된 estimator 반환
estimator = grid_dtree.best_estimator_

# GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨
pred = estimator.predict(X_test)
print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))

```

### 6. 데이터 인코딩 (원-핫 인코딩)

사이킷런의 ML 모델을 적용하기 전에 아래 2가지 사항을 필수로 처리해야한다.<br>

1. 결손값<br>
1. 문자열 인코딩<br>

#### 6-1. 레이블 인코딩

카테고리 피쳐를 코ㄷ형 숫자 값으로 변환하는 방법<br>

**숫자형 레이블의 크고 작음에 대한 특성이 작용하는 문제점이 있다.**

##### 사용법
```python
from sklearn.preprocessing import LabelEncoder

items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']

# LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행. 
encoder = LabelEncoder()
encoder.fit(items)
labels = encoder.transform(items)
print('인코딩 변환값:',labels)

print('인코딩 클래스:',encoder.classes_)

print('디코딩 원본 값:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3]))
```

#### 6-2. 원-핫 인코딩

레이블 인코딩의 숫자형 레이블의 크고 작음에 대한 특성 작용 문제를 해결하기 위한 방법이다.<br>

피쳐 값의 유형에 따라 새로운 피쳐를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방법이다.<br>

**사이킷런의 `OneHotEncoder`를 사용하기 위해서는 모든 문자열 값이 숫자형 값으로 변환되어야 하고, 2차원 데이터가 필요하다.**
**판다스의 `get_dummies()` 메서드를 사용하면 카테고리 값을 숫자형으로 변환하지 않고 원-핫 인코딩 할 수 있다.**

##### 사용법
```python

from sklearn.preprocessing import OneHotEncoder
import numpy as np

items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']

# 먼저 숫자값으로 변환을 위해 LabelEncoder로 변환합니다. 
encoder = LabelEncoder()
encoder.fit(items)
labels = encoder.transform(items)
# 2차원 데이터로 변환합니다. 
labels = labels.reshape(-1,1)

# 원-핫 인코딩을 적용합니다. 
oh_encoder = OneHotEncoder()
oh_encoder.fit(labels)
oh_labels = oh_encoder.transform(labels)
print('원-핫 인코딩 데이터')
print(oh_labels.toarray())
print('원-핫 인코딩 데이터 차원')
print(oh_labels.shape)
```
```python
import pandas as pd

df = pd.DataFrame({'item':['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서'] })
pd.get_dummies(df)

```

### 7. 피처 스케일링과 정규화

서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업을 **피쳐 스케일링**이라고 한다.<br>

대표적인 방법으로 표준화(Standardization)와 정규화(Normalization)가 있다.<br>

- 표준화(Standardization)<br>
    : 데이터의 피쳐 각각이 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환한다.<br>
    ![표준화 수식]()
    <br>
- 정규화(Normalization)<br>
    : 서로 다른 피쳐의 크기를 통일하기 위해 크기를 변환해주는 개념이다.<br>
    ![정규화 수식]()
    <br>
    **개별 데이터의 크기를 모두 똑같은 단위롤 변경하는 것과 동일하다.**

**사이킷런의 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 있다.**<br>
**사이킷런의 Normalizer 모듈은 선형대수에서의 정규화 개변이 적용되었으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미한다.**<br>
**해당 책에서는 일반적인 의미의 표준화와 정규화를 피쳐 스케일링으로 통칭하고, 선형대수 개념의 정규화를 벡터 정규화로 지칭한다.**

#### 7-1. StandardScaler

표준화를 쉽게 지원하기 위한 클래스로 개별 피쳐를 평균이 9이고, 분산이 1인 값으로 변환해준다.<br>

**사이킷런에서 구현한 RBF 커널을 이용하는 서포트 벡터 머신(SVM), 선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression)은 데이터가 가우시간 분포를 가지고 있다고 가정하고 구현되었기 때문에 사전에 표준화를 적용하는 것이 중요하다.**<br>

##### 사용법
```python
from sklearn.datasets import load_iris
import pandas as pd
# 붓꽃 데이터 셋을 로딩하고 DataFrame으로 변환합니다. 
iris = load_iris()
iris_data = iris.data
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)

print('feature 들의 평균 값')
print(iris_df.mean())
print('\nfeature 들의 분산 값')
print(iris_df.var())

from sklearn.preprocessing import StandardScaler

# StandardScaler객체 생성
scaler = StandardScaler()
# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

#transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
print('feature 들의 평균 값')
print(iris_df_scaled.mean())
print('\nfeature 들의 분산 값')
print(iris_df_scaled.var())
```

#### 7-2. MinMaxScaler

데이터 값을 0과 1 사이의 범위 값으로 변환한다.<br>
데이터의 분포가 가우시안 분포가 아닐 경우에 MinMaxScaler을 적용할 수 있다.<br>

##### 사용법
```python
from sklearn.preprocessing import MinMaxScaler

# MinMaxScaler객체 생성
scaler = MinMaxScaler()
# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
print('feature들의 최소 값')
print(iris_df_scaled.min())
print('\nfeature들의 최대 값')
print(iris_df_scaled.max())
```

#### 7-3. 스케일링시 유의사항

학습 데이터로 fit이 적용된 스켕일 기준 정보를 그대로 테스트 데이터에 적용해야하며<br>
그렇지 않고 테스트 데이터로 다시 새로운 스케일링 기준 정보를 만들 경우 올바른 예측 결과를 도출하지 못할 수 있다.<br>

따라서 아래 2가지 사항을 유의해야 한다.<br>

1. 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리<br>
1. 1번이 여의치 않다면 테스트 데이터 변환시에는 `fit()`이나 `fit_transform()`을 적용하지 않고 학습 데이터로 이미 `fit()`된 sCALER 객체를 이용해 `transform()`으로 변환<br>

##### 문제 발생 예시
```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 학습 데이터는 0 부터 10까지, 테스트 데이터는 0 부터 5까지 값을 가지는 데이터 세트로 생성
# Scaler클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경
train_array = np.arange(0, 11).reshape(-1, 1)
test_array =  np.arange(0, 6).reshape(-1, 1)


# 최소값 0, 최대값 1로 변환하는 MinMaxScaler객체 생성
scaler = MinMaxScaler()
# fit()하게 되면 train_array 데이터의 최소값이 0, 최대값이 10으로 설정.  
scaler.fit(train_array)
# 1/10 scale로 train_array 데이터 변환함. 원본 10-> 1로 변환됨.
train_scaled = scaler.transform(train_array)
 
print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))
print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))

# 앞에서 생성한 MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최소값이 0, 최대값이 5으로 설정됨 
scaler.fit(test_array)
# 1/5 scale로 test_array 데이터 변환함. 원본 5->1로 변환.  
test_scaled = scaler.transform(test_array)
# train_array 변환 출력
print('원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))
print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))

scaler = MinMaxScaler()
scaler.fit(train_array)
train_scaled = scaler.transform(train_array)
print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))
print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))

# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform() 만으로 변환해야 함. 
test_scaled = scaler.transform(test_array)
print('\n원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))
print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))
```


